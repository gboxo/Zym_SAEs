_wandb:
    value:
        cli_version: 0.19.6
        m: []
        python_version: 3.12.3
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 105
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 105
            "3":
                - 13
                - 16
                - 23
                - 55
            "4": 3.12.3
            "5": 0.19.6
            "6": 4.49.0
            "8":
                - 5
            "12": 0.19.6
            "13": linux-x86_64
act_store_device:
    value: cpu
activation_fn:
    value: topk
activation_fn_kwargs:
    value:
        k: 100
adam_beta1:
    value: 0.9
adam_beta2:
    value: 0.999
apply_b_dec_to_input:
    value: false
architecture:
    value: topk
autocast:
    value: false
autocast_lm:
    value: false
b_dec_init_method:
    value: zeros
cached_activations_path:
    value: null
checkpoint_path:
    value: checkpoints/j49fx2s3
compile_llm:
    value: false
compile_sae:
    value: false
context_size:
    value: 128
d_in:
    value: 1280
d_sae:
    value: 10240
dataset_path:
    value: nferruz/UR50_2021_04
dataset_trust_remote_code:
    value: true
dead_feature_threshold:
    value: 1e-08
dead_feature_window:
    value: 1000
decoder_heuristic_init:
    value: true
decoder_orthogonal_init:
    value: false
device:
    value: cpu
dtype:
    value: float32
eval_batch_size_prompts:
    value: null
eval_every_n_wandb_logs:
    value: 20
exclude_special_tokens:
    value: false
expansion_factor:
    value: 8
feature_sampling_window:
    value: 1000
finetuning_method:
    value: null
finetuning_tokens:
    value: 0
from_pretrained_path:
    value: null
hook_eval:
    value: NOT_IN_USE
hook_head_index:
    value: null
hook_layer:
    value: 10
hook_name:
    value: blocks.10.hook_resid_pre
init_encoder_as_decoder_transpose:
    value: true
is_dataset_tokenized:
    value: true
jumprelu_bandwidth:
    value: 0.001
jumprelu_init_threshold:
    value: 0.001
l1_coefficient:
    value: 0.001
l1_warm_up_steps:
    value: 20000
llm_compilation_mode:
    value: null
log_activations_store_to_wandb:
    value: false
log_optimizer_state_to_wandb:
    value: false
log_to_wandb:
    value: true
lp_norm:
    value: 1
lr:
    value: 6e-06
lr_decay_steps:
    value: 20000
lr_end:
    value: 6e-07
lr_scheduler_name:
    value: constant
lr_warm_up_steps:
    value: 5000
model_class_name:
    value: HookedTransformer
model_from_pretrained_kwargs:
    value:
        center_writing_weights: false
model_name:
    value: nferruz/ProtGPT2
mse_loss_normalization:
    value: none
n_batches_in_buffer:
    value: 8
n_checkpoints:
    value: 0
n_eval_batches:
    value: 10
n_restart_cycles:
    value: 1
noise_scale:
    value: 0
normalize_activations:
    value: expected_average_only_in
normalize_sae_decoder:
    value: false
prepend_bos:
    value: true
resume:
    value: false
run_name:
    value: 10240-L1-0.001-LR-6e-06-Tokens-1.024e+08
sae_compilation_mode:
    value: null
sae_lens_training_version:
    value: 5.4.2
sae_lens_version:
    value: 5.4.2
scale_sparsity_penalty_by_decoder_norm:
    value: true
seed:
    value: 42
seqpos_slice:
    value:
        - null
store_batch_size_prompts:
    value: 16
streaming:
    value: true
tokens_per_buffer:
    value: 1048576
train_batch_size_tokens:
    value: 1024
training_tokens:
    value: 102400000
use_cached_activations:
    value: false
use_ghost_grads:
    value: false
verbose:
    value: true
wandb_entity:
    value: null
wandb_id:
    value: null
wandb_log_frequency:
    value: 30
wandb_project:
    value: protGPT_SAE
