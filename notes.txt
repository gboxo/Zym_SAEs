
"""
# ========== Line by line executtion ============
from torch import layer_norm
from torch.nn import functional as F    
wte = cache["hook_embed"]
rs_0_pre = cache["blocks.0.hook_resid_pre"]
weight = model.state_dict()["blocks.0.ln1.w"]
bias = model.state_dict()["blocks.0.ln1.b"]
x = rs_0_pre
y = rs_0_pre

y = F.layer_norm(y, weight.shape, weight, bias, 1e-5)
x = x - x.mean(-1, keepdim=True)  # [batch, pos, length]
scale = (x.pow(2).mean(-1, keepdim=True) + 1e-5).sqrt()
x = x / scale  # [batch, pos, length]
ln_scale_0 = cache["blocks.0.ln1.hook_scale"]
ln_normalized_0 = cache["blocks.0.ln1.hook_normalized"]


# Residual stream Layer 0
print("==============")
resid_pre = cache["blocks.0.hook_resid_pre"]
print("Resid pre Layer 0",resid_pre.sum())
attn_out = cache["blocks.0.hook_attn_out"]
print("Attn out Layer 0",attn_out.sum())
resid_post = cache["blocks.0.hook_resid_post"]
print("Resid post Layer 0",resid_post.sum())

# Residual stream Layer 0
print("==============")
resid_pre = cache["blocks.1.hook_resid_pre"]
print("Resid pre Layer 1",resid_pre.sum())
resid_post = cache["blocks.1.hook_resid_post"]
print("Resid post Layer 1",resid_post.sum())

# Keys and Queries (Layer 1)
print("=====================")
W_Q = cache["blocks.0.attn.hook_q"]
print("Layer 0 W_Q",W_Q.sum())
W_K = cache["blocks.0.attn.hook_k"]
print("Layer 0 W_K",W_K.sum())
W_V = cache["blocks.0.attn.hook_v"]
print("Layer 0 W_V",W_V.sum())
# Keys and Queries
W_Q = cache["blocks.0.attn.hook_rot_q"]
print("Layer 0 W_Q_rot",W_Q.sum())
W_K = cache["blocks.0.attn.hook_rot_k"]
print("Layer 0 W_K_rot",W_K.sum())
print("=====================")
# Keys and Queries (Layer 1)
W_Q = cache["blocks.1.attn.hook_q"]
print("Layer 1 W_Q",W_Q.sum())
W_K = cache["blocks.1.attn.hook_k"]
print("Layer 1 W_K",W_K.sum())
W_V = cache["blocks.1.attn.hook_v"]
print("Layer 1 W_V",W_V.sum())
# Keys and Queries
W_Q = cache["blocks.1.attn.hook_rot_q"]
print("Layer 1 W_Q_rot",W_Q.sum())
W_K = cache["blocks.1.attn.hook_rot_k"]
print("Layer 1 W_K_rot",W_K.sum())

"""







"""

    cfg_dict = {
        "d_model": hf_config.hidden_size,
        "d_head": hf_config.hidden_size // hf_config.num_attention_heads,
        "n_heads": hf_config.num_attention_heads,
        "n_key_value_heads": hf_config.num_key_value_heads,
        "d_mlp": hf_config.intermediate_size,
        "n_layers": hf_config.num_hidden_layers,
        "n_ctx": 2048*3,  # Capped bc the actual ctx length is 30k and the attn mask would be too big
        "eps": hf_config.rms_norm_eps,
        "d_vocab": hf_config.vocab_size,
        "act_fn": hf_config.hidden_act,
        "use_attn_scale": True,
        "initializer_range": hf_config.initializer_range,
        "normalization_type": "RMS",
        "positional_embedding_type": "rotary",
        "rotary_base": int(hf_config.rope_theta),
        "rotary_adjacent_pairs": False,
        "rotary_dim": hf_config.hidden_size // hf_config.num_attention_heads,
        "tokenizer_prepends_bos": True,
        "final_rms": True,
        "gated_mlp": True,
        "default_prepend_bos": False,
    }

"""


